Fine-tune 90M model

parlai train_model -t blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues  -m transformer/generator --multitask-weights 1,3,3,3 --init-model zoo:blender/blender_90M/model --dict-file zoo:blender/blender_90M/model.dict --embedding-size 512 --n-layers 8 --ffn-size 2048 --dropout 0.1 --n-heads 16 --learn-positional-embeddings True --n-positions 512 --variant xlm --activation gelu --skip-generation True --fp16 True --text-truncate 512 --label-truncate 128 --dict-tokenizer bpe --dict-lower True -lr 1e-06 --optimizer adamax --lr-scheduler reduceonplateau --gradient-clip 0.1 -veps 0.25 --betas 0.9,0.999 --update-freq 1 --attention-dropout 0.0 --relu-dropout 0.0 --skip-generation True -vp 15 -stim 60 -vme 20000 -bs 16 -vmt ppl -vmm min --save-after-valid True --model-file D:\Bot_work\ParlAI\tmp\test_train_90M

safe_interative self trained:

!cd ParlAI; python parlai/scripts/safe_interactive.py -t blended_skill_talk -mf /tmp/test_train_90M --fp16 True --beam-block-full-context True --include-personas False --include-context False --beam-min-length 10